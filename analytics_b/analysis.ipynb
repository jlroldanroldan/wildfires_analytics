{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import re  # regular expressions\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import rasterio as rio\n",
    "from rasterio.plot import plotting_extent\n",
    "import geopandas as gpd\n",
    "import earthpy as et\n",
    "import earthpy.plot as ep\n",
    "import earthpy.spatial as es\n",
    "import earthpy.mask as em\n",
    "\n",
    "import rioxarray as rxr\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from queue import Queue\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(os.path.join(et.io.HOME, 'BD', 'BA_DATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "d_limit = 1200\n",
    "tem_conversion = 0.02 # convert to Kelvin temperature\n",
    "vege_fillvalue = -3000\n",
    "vege_lowerest = -2000\n",
    "NE_fillvalue = 32767\n",
    "tem_fillvalue = 0\n",
    "radius = 50\n",
    "\n",
    "vege_len = 457\n",
    "tem_len = 914\n",
    "NE_len = 914\n",
    "thermal_len = 914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vege_fs84 = glob('VegeData_16day/*h08v04*')\n",
    "vege_fs85 = glob('VegeData_16day/*h08v05*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_fs84 = glob('TemData_8day/*h08v04*')\n",
    "tem_fs85 = glob('TemData_8day/*h08v05*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NE_fs84 = glob('NE_Data_8day/*h08v04*')\n",
    "NE_fs85 = glob('NE_Data_8day/*h08v05*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "thermal_fs84 = glob('ThermalData_8day/*h08v04*')\n",
    "thermal_fs85 = glob('ThermalData_8day/*h08v05*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NE_fs85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(NE_fs84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tem_fs85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick any location, divide the neighborhood into 10 regions of interest in various directions, each region is within 50 km, 36 degree and 50 km sector as a region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sector_mask(shape,centre,radius,angle_range):\n",
    "    \"\"\"\n",
    "    Return a boolean mask for a circular sector. The start/stop angles in  \n",
    "    `angle_range` should be given in clockwise order.\n",
    "    \"\"\"\n",
    "\n",
    "    x,y = np.ogrid[:shape[0],:shape[1]]\n",
    "    cx,cy = centre\n",
    "    tmin,tmax = np.deg2rad(angle_range)\n",
    "\n",
    "    # ensure stop angle > start angle\n",
    "    if tmax < tmin:\n",
    "            tmax += 2*np.pi\n",
    "\n",
    "    # convert cartesian --> polar coordinates\n",
    "    r2 = (x-cx)*(x-cx) + (y-cy)*(y-cy)\n",
    "    theta = np.arctan2(x-cx,y-cy) - tmin\n",
    "\n",
    "    # wrap angles between 0 and 2*pi\n",
    "    theta %= (2*np.pi)\n",
    "\n",
    "    # circular mask\n",
    "    circmask = r2 <= radius*radius\n",
    "\n",
    "    # angular mask\n",
    "    anglemask = theta <= (tmax-tmin)\n",
    "\n",
    "    return circmask*anglemask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_set(matrix, fill_value, new_value, row, colmn):\n",
    "    \"\"\"\n",
    "    Return a feature set of a single pixel with row number ROW and column number COLUMN in the 1200 * 1200 matrix MATRIX.\n",
    "    Feature set is computed based on the average value in 10 regions in one time frame from FILENAMES.\n",
    "    Fill_value will be replaced by new_value in the feature matrix.\n",
    "    \"\"\"\n",
    "    feature_set = []\n",
    "    for i in range(10):\n",
    "        mask = sector_mask(matrix.shape, (row, column), radius, (i, i + 36))\n",
    "        sector = matrix[mask]\n",
    "        sector[:][sector[:] == fill_value] = new_value\n",
    "        mean_value = np.mean(sector)\n",
    "        feature_set.append(mean_value)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vege_feature_set(matrix, row, colmn):\n",
    "    \"\"\"\n",
    "    Return a vegetation indices feature set of a single pixel with row number ROW and column number COLUMN \n",
    "    in the 1200 * 1200 matrix MATRIX.\n",
    "    Feature set is computed based on the average value in 10 regions in one time frame from FILENAMES.\n",
    "    \"\"\"\n",
    "    feature_set = []\n",
    "    for i in range(10):\n",
    "        mask = sector_mask(matrix.shape, (row, column), radius, (i, i + 36))\n",
    "        sector = matrix[mask]\n",
    "        sector[:][sector[:] == vege_fillvalue] = vege_lowerest # change fillvalue\n",
    "        mean_value = np.mean(sector)\n",
    "        feature_set.append(mean_value)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_thermal_feature_set(matrix, row, colmn):\n",
    "    \"\"\"\n",
    "    Return a thermal anomalies feature set of a single pixel with row number ROW and column number COLUMN \n",
    "    in the 1200 * 1200 matrix MATRIX.\n",
    "    Feature set is computed based on the sum of value in 10 regions in one time frame from FILENAMES.\n",
    "    \"\"\"\n",
    "    feature_set = []\n",
    "    for i in range(10):\n",
    "        mask = sector_mask(matrix.shape, (row, column), radius, (i, i + 36))\n",
    "        sector = matrix[mask]\n",
    "        sector[:][sector[:] < 7] = 0\n",
    "        sector[:][sector[:] >= 7] = 1\n",
    "        sum_value = np.sum(sector)\n",
    "        feature_set.append(sum_value)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tem_feature_set(matrix, row, column):\n",
    "    \"\"\"\n",
    "    Return a surface temperature feature set of a single pixel with row number ROW and column number COLUMN \n",
    "    in the 1200 * 1200 matrix MATRIX.\n",
    "    Feature set is computed based on the mean of value in 10 regions in one time frame from FILENAMES.\n",
    "    \"\"\"\n",
    "    feature_set = [] # use array instead\n",
    "    for i in range(10):\n",
    "        mask = sector_mask(matrix.shape, (row, column), radius, (i, i + 36))\n",
    "        sector = matrix[mask]\n",
    "        #sector = sector[sector[:] != 0] # drop fillvalue\n",
    "        mean_value = np.mean(sector)\n",
    "        feature_set.append(mean_value)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NE_feature_set(matrix, row, column):\n",
    "    \"\"\"\n",
    "    Return a NE feature set of a single pixel with row number ROW and column number COLUMN \n",
    "    in the 1200 * 1200 matrix MATRIX.\n",
    "    Feature set is computed based on the mean of value in 10 regions in one time frame from FILENAMES.\n",
    "    \"\"\"\n",
    "    feature_set = []\n",
    "    for i in range(10):\n",
    "        mask = sector_mask(matrix.shape, (row, column), radius, (i, i + 36))\n",
    "        sector = matrix[mask]\n",
    "        mean_value = np.mean(sector)\n",
    "        feature_set.append(mean_value)\n",
    "    return feature_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 16-day hdf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16-day for vegetation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vege_features(file, sds, fill_value, new_value, radius):\n",
    "    \"\"\"\n",
    "    Return feature sets of all pixels in the file FILE.\n",
    "    SDS is NDVI or EVI.\n",
    "    \n",
    "    \"\"\"\n",
    "    all_bands = []\n",
    "    with rio.open(file) as dataset:\n",
    "        for name in dataset.subdatasets:\n",
    "            if re.search(sds, name):\n",
    "                with rio.open(name) as subdataset:\n",
    "                    modis_meta = subdataset.profile\n",
    "                    all_bands.append(subdataset.read(1))\n",
    "    vege_modis = np.stack(all_bands)\n",
    "    vege_matrix = vege_modis[0].reshape(d_limit,d_limit)\n",
    "    feature_sets = np.zeros((d_limit, d_limit))\n",
    "    for r in range(d_limit):\n",
    "        for c in range(d_limit):\n",
    "            temp = compute_feature_set(vege_martrix, fill_value, new_value, r, c)\n",
    "            feature_sets[r][c] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vege_features(filenames, num_file, sds, fill_value, new_value, radius):\n",
    "    \"\"\"\n",
    "    Return vegetation indice feature sets for all files in FILENAMES\n",
    "    \"\"\"\n",
    "    all_feature_sets = np.zeros((num_file, d_limit, d_limit))\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for file in filenames:\n",
    "        temp = vege_features(file, sds, file_value, new_value, radius)\n",
    "        all_feature_sets[index] = temp\n",
    "        index += 1\n",
    "    return all_feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin(m, shape):\n",
    "    \"\"\"\n",
    "    Reshape the input matrix A to the shape SHAPE.\n",
    "    \"\"\"\n",
    "    sh = shape[0],m.shape[0]//shape[0],shape[1],m.shape[1]//shape[1]\n",
    "    return m.reshape(sh).mean(-1).mean(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process 8-day files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8-day for thermal_data, tem_data and NE_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NE_Data_8day\\\\MOD16A2.A2001001.h08v05.006.2017068145443.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001009.h08v05.006.2017068172948.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001017.h08v05.006.2017068204420.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001025.h08v05.006.2017068232911.hdf']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_fs85[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NE_Data_8day\\\\MOD16A2.A2001033.h08v05.006.2017069023644.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001041.h08v05.006.2017069060452.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001049.h08v05.006.2017069091436.hdf',\n",
       " 'NE_Data_8day\\\\MOD16A2.A2001057.h08v05.006.2017069124001.hdf']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NE_fs85[4:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features_8day(filenames, num_file, radius, feature_type):\n",
    "    \"\"\"\n",
    "    Combine every two 8-day files to compute feature sets. There are in total NUM_FILES in filenames. Used for 8-day datasets.\n",
    "    Use a queue to help iterate through every two files.\n",
    "    Input filenames is the result from using glob command\n",
    "    Result is a 3d np array with all the SDS features using 10-region method\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    sds1 = None\n",
    "    sds2 = None\n",
    "    if feature_type == \"thermal\":\n",
    "        sds1 = \"FireMask\"\n",
    "    elif feature_type == \"tem\":\n",
    "        sds1 = \"LST_Day\"\n",
    "    elif feature_type == \"NE\":\n",
    "        sds1 = \":ET_500m\"\n",
    "        sds2 = \"PET\"\n",
    "    else:\n",
    "        print(\"No such feature for 8-day\")\n",
    "        return\n",
    "    print(2)\n",
    "    file_count = num_file // 2\n",
    "    all_feature_sets = np.zeros((file_count, 1080, 1080, 10))\n",
    "    q = Queue(1000)\n",
    "    print(3)\n",
    "    for filename in filenames:\n",
    "        q.put(filename)\n",
    "    print(4)\n",
    "    for i in range (file_count):\n",
    "        f1 = q.get()\n",
    "        f2 = q.get()\n",
    "        f1_bands = []\n",
    "        f2_bands = []\n",
    "        f1_bands_se = []\n",
    "        f2_bands_se = []\n",
    "        # open two files in a round\n",
    "        with rio.open(f1) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f1_bands.append(subdataset.read(1))\n",
    "                if feature_type == \"NE\":\n",
    "                    if re.search(sds2, name):\n",
    "                        with rio.open(name) as subdataset:\n",
    "                            modis_meta = subdataset.profile\n",
    "                            f1_bands_se.append(subdataset.read(1))\n",
    "        f1_modis = np.stack(f1_bands)\n",
    "        f1_matrix = f1_modis[0].reshape(d_limit,d_limit)\n",
    "        if feature_type == \"NE\":\n",
    "            f1_modis_se = np.stack(f1_bands_se)\n",
    "            f1_matrix_se = f1_modis_se[0].reshape(d_limit,d_limit)\n",
    "        print(5)\n",
    "        with rio.open(f2) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f2_bands.append(subdataset.read(1))\n",
    "                if feature_type == \"NE\":\n",
    "                    if re.search(sds2, name):\n",
    "                        with rio.open(name) as subdataset:\n",
    "                            modis_meta = subdataset.profile\n",
    "                            f2_bands_se.append(subdataset.read(1))\n",
    "        f2_modis = np.stack(f2_bands)\n",
    "        f2_matrix = f2_modis[0].reshape(d_limit,d_limit)\n",
    "        if feature_type == \"NE\":\n",
    "            f2_modis_se = np.stack(f2_bands_se)\n",
    "            f2_matrix_se = f2_modis_se[0].reshape(d_limit,d_limit)\n",
    "        # combine two matrices\n",
    "        if feature_type == \"NE\":\n",
    "            f1_matrix = f1_matrix_se - f1_matrix\n",
    "            f2_matrix = f2_matrix_se - f2_matrix\n",
    "        combined_matrix = (f1_matrix + f2_matrix) / 2\n",
    "        if feature_type == \"tem\":\n",
    "            combined_matrix = combined_matrix * tem_conversion\n",
    "        feature_sets = np.zeros((1080, 1080))\n",
    "        print(6)\n",
    "        for r in range(60, 1140):\n",
    "            for c in range(60, 1140):\n",
    "                if feature_type == \"thermal\":\n",
    "                    temp = compute_thermal_feature_set(combined_matrix, r, c)\n",
    "                elif feature_type == \"tem\":\n",
    "                    temp = compute_tem_feature_set(combined_matrix, r, c)\n",
    "                    print(temp)\n",
    "                elif feature_type == \"vege\":\n",
    "                    temp = compute_vege_feature_set(combined_matrix, r, c)\n",
    "                else:\n",
    "                    temp = compute_NE_feature_set(combined_matrix, r, c)\n",
    "                feature_sets[r][c] = np.copy(temp)\n",
    "        \n",
    "        all_feature_sets[i] = np.copy(feature_sets)\n",
    "        return all_feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_TEMfeatures_8day(filenames, num_file, radius):\n",
    "    \"\"\"\n",
    "    Combine every two 8-day files to compute feature sets. There are in total NUM_FILES in filenames. Used for 8-day datasets.\n",
    "    Use a queue to help iterate through every two files.\n",
    "    Input filenames is the result from using glob command\n",
    "    Result is a 3d np array with all the SDS features using 10-region method\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    sds1 = \"LST_Day\"\n",
    "    print(2)\n",
    "    file_count = num_file // 2\n",
    "    all_feature_sets = np.zeros((file_count, 1080, 1080, 10))\n",
    "    q = Queue(1000)\n",
    "    print(3)\n",
    "    for filename in filenames:\n",
    "        q.put(filename)\n",
    "    print(4)\n",
    "    for i in range (file_count):\n",
    "        f1 = q.get()\n",
    "        f2 = q.get()\n",
    "        f1_bands = []\n",
    "        f2_bands = []\n",
    "        f1_bands_se = []\n",
    "        f2_bands_se = []\n",
    "        # open two files in a round\n",
    "        with rio.open(f1) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f1_bands.append(subdataset.read(1))\n",
    "        f1_modis = np.stack(f1_bands)\n",
    "        f1_matrix = f1_modis[0].reshape(d_limit,d_limit)\n",
    "        print(5)\n",
    "        with rio.open(f2) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f2_bands.append(subdataset.read(1))\n",
    "        f2_modis = np.stack(f2_bands)\n",
    "        f2_matrix = f2_modis[0].reshape(d_limit,d_limit)\n",
    "        # combine two matrices\n",
    "        combined_matrix = (f1_matrix + f2_matrix) / 2\n",
    "        combined_matrix = combined_matrix * tem_conversion\n",
    "        feature_sets = np.zeros((1080, 1080, 10))\n",
    "        print(6)\n",
    "        for r in range(60, 1140):\n",
    "            for c in range(60, 1140):\n",
    "                print((r-60,c-60))\n",
    "                temp = compute_tem_feature_set(combined_matrix, r, c)\n",
    "                feature_sets[r-60][c-60] = np.copy(temp)\n",
    "        \n",
    "        all_feature_sets[i] = np.copy(feature_sets)\n",
    "        return all_feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_THERMALfeatures_8day(filenames, num_file, radius):\n",
    "    \"\"\"\n",
    "    Combine every two 8-day files to compute feature sets. There are in total NUM_FILES in filenames. Used for 8-day datasets.\n",
    "    Use a queue to help iterate through every two files.\n",
    "    Input filenames is the result from using glob command\n",
    "    Result is a 3d np array with all the SDS features using 10-region method\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    sds1 = \"FireMask\"\n",
    "    print(2)\n",
    "    file_count = num_file // 2\n",
    "    all_feature_sets = np.zeros((file_count, 1080, 1080, 10))\n",
    "    q = Queue(file_count)\n",
    "    print(3)\n",
    "    for filename in filenames:\n",
    "        q.put(filename)\n",
    "    print(4)\n",
    "    for i in range (file_count):\n",
    "        f1 = q.get()\n",
    "        f2 = q.get()\n",
    "        f1_bands = []\n",
    "        f2_bands = []\n",
    "        f1_bands_se = []\n",
    "        f2_bands_se = []\n",
    "        # open two files in a round\n",
    "        with rio.open(f1) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f1_bands.append(subdataset.read(1))\n",
    "        f1_modis = np.stack(f1_bands)\n",
    "        f1_matrix = f1_modis[0].reshape(d_limit,d_limit)\n",
    "        print(5)\n",
    "        with rio.open(f2) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f2_bands.append(subdataset.read(1))\n",
    "        f2_modis = np.stack(f2_bands)\n",
    "        f2_matrix = f2_modis[0].reshape(d_limit,d_limit)\n",
    "        # combine two matrices\n",
    "        combined_matrix = (f1_matrix + f2_matrix) / 2\n",
    "        feature_sets = np.zeros((1080, 1080))\n",
    "        print(6)\n",
    "        for r in range(60, 1140):\n",
    "            for c in range(60, 1140):\n",
    "                print((r-60,c-60))\n",
    "                temp = compute_thermal_feature_set(combined_matrix, r, c)\n",
    "                feature_sets[r-60][c-60] = np.copy(temp)\n",
    "        \n",
    "        all_feature_sets[i] = np.copy(feature_sets)\n",
    "        return all_feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_NEfeatures_8day(filenames, num_file, radius):\n",
    "    \"\"\"\n",
    "    Combine every two 8-day files to compute feature sets. There are in total NUM_FILES in filenames. Used for 8-day datasets.\n",
    "    Use a queue to help iterate through every two files.\n",
    "    Input filenames is the result from using glob command\n",
    "    Result is a 3d np array with all the SDS features using 10-region method\n",
    "    \"\"\"\n",
    "    print(1)\n",
    "    sds1 = \":ET_500m\"\n",
    "    sds2 = \"PET\"\n",
    "    print(2)\n",
    "    file_count = num_file // 2\n",
    "    all_feature_sets = np.zeros((file_count, 1080, 1080, 10))\n",
    "    q = Queue(num_file)\n",
    "    print(3)\n",
    "    for filename in filenames:\n",
    "        q.put(filename)\n",
    "    print(4)\n",
    "    for i in range (file_count):\n",
    "        f1 = q.get()\n",
    "        f2 = q.get()\n",
    "        f1_bands = []\n",
    "        f2_bands = []\n",
    "        f1_bands_se = []\n",
    "        f2_bands_se = []\n",
    "        # open two files in a round\n",
    "        with rio.open(f1) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f1_bands.append(subdataset.read(1))\n",
    "                if re.search(sds2, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f1_bands_se.append(subdataset.read(1))\n",
    "        f1_modis = np.stack(f1_bands)\n",
    "        f1_matrix = f1_modis[0].reshape(d_limit,d_limit)\n",
    "        f1_modis_se = np.stack(f1_bands_se)\n",
    "        f1_matrix_se = f1_modis_se[0].reshape(d_limit,d_limit)\n",
    "        print(5)\n",
    "        with rio.open(f2) as dataset:\n",
    "            for name in dataset.subdatasets:\n",
    "                if re.search(sds1, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f2_bands.append(subdataset.read(1))\n",
    "                if re.search(sds2, name):\n",
    "                    with rio.open(name) as subdataset:\n",
    "                        modis_meta = subdataset.profile\n",
    "                        f2_bands_se.append(subdataset.read(1))\n",
    "        f2_modis = np.stack(f2_bands)\n",
    "        f2_matrix = f2_modis[0].reshape(d_limit,d_limit)\n",
    "        f2_modis_se = np.stack(f2_bands_se)\n",
    "        f2_matrix_se = f2_modis_se[0].reshape(d_limit,d_limit)\n",
    "        # combine two matrices\n",
    "        f1_matrix = f1_matrix_se - f1_matrix\n",
    "        f2_matrix = f2_matrix_se - f2_matrix\n",
    "        combined_matrix = (f1_matrix + f2_matrix) / 2\n",
    "        feature_sets = np.zeros((1080, 1080))\n",
    "        print(6)\n",
    "        for r in range(60, 1140):\n",
    "            for c in range(60, 1140):\n",
    "                print((r-60,c-60))\n",
    "                temp = compute_NE_feature_set(combined_matrix, r, c)\n",
    "                feature_sets[r-60][c-60] = np.copy(temp)\n",
    "        \n",
    "        all_feature_sets[i] = np.copy(feature_sets)\n",
    "        return all_feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within a region, exact a small set of features, and ask is there any fire observed in the region in the past 16 days. Consider the intensity of fire, number of pixels with fire (a pixel with fire for 3 days are considered as 3 fire), for a region in the past 16 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "914"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tem_fs85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "457"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vege_fs85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "(0, 0)\n",
      "(0, 1)\n",
      "(0, 2)\n",
      "(0, 3)\n",
      "(0, 4)\n",
      "(0, 5)\n",
      "(0, 6)\n",
      "(0, 7)\n",
      "(0, 8)\n",
      "(0, 9)\n",
      "(0, 10)\n",
      "(0, 11)\n",
      "(0, 12)\n",
      "(0, 13)\n",
      "(0, 14)\n",
      "(0, 15)\n",
      "(0, 16)\n",
      "(0, 17)\n",
      "(0, 18)\n",
      "(0, 19)\n",
      "(0, 20)\n",
      "(0, 21)\n",
      "(0, 22)\n",
      "(0, 23)\n",
      "(0, 24)\n",
      "(0, 25)\n",
      "(0, 26)\n",
      "(0, 27)\n",
      "(0, 28)\n",
      "(0, 29)\n",
      "(0, 30)\n",
      "(0, 31)\n",
      "(0, 32)\n",
      "(0, 33)\n",
      "(0, 34)\n",
      "(0, 35)\n",
      "(0, 36)\n",
      "(0, 37)\n",
      "(0, 38)\n",
      "(0, 39)\n",
      "(0, 40)\n",
      "(0, 41)\n",
      "(0, 42)\n",
      "(0, 43)\n",
      "(0, 44)\n",
      "(0, 45)\n",
      "(0, 46)\n",
      "(0, 47)\n",
      "(0, 48)\n",
      "(0, 49)\n",
      "(0, 50)\n",
      "(0, 51)\n",
      "(0, 52)\n",
      "(0, 53)\n",
      "(0, 54)\n",
      "(0, 55)\n",
      "(0, 56)\n",
      "(0, 57)\n",
      "(0, 58)\n",
      "(0, 59)\n",
      "(0, 60)\n",
      "(0, 61)\n",
      "(0, 62)\n",
      "(0, 63)\n",
      "(0, 64)\n",
      "(0, 65)\n",
      "(0, 66)\n",
      "(0, 67)\n",
      "(0, 68)\n",
      "(0, 69)\n",
      "(0, 70)\n",
      "(0, 71)\n",
      "(0, 72)\n",
      "(0, 73)\n",
      "(0, 74)\n",
      "(0, 75)\n",
      "(0, 76)\n",
      "(0, 77)\n",
      "(0, 78)\n",
      "(0, 79)\n",
      "(0, 80)\n",
      "(0, 81)\n",
      "(0, 82)\n",
      "(0, 83)\n",
      "(0, 84)\n",
      "(0, 85)\n",
      "(0, 86)\n",
      "(0, 87)\n",
      "(0, 88)\n",
      "(0, 89)\n",
      "(0, 90)\n",
      "(0, 91)\n",
      "(0, 92)\n",
      "(0, 93)\n",
      "(0, 94)\n",
      "(0, 95)\n",
      "(0, 96)\n",
      "(0, 97)\n",
      "(0, 98)\n",
      "(0, 99)\n",
      "(0, 100)\n",
      "(0, 101)\n",
      "(0, 102)\n",
      "(0, 103)\n",
      "(0, 104)\n",
      "(0, 105)\n",
      "(0, 106)\n",
      "(0, 107)\n",
      "(0, 108)\n",
      "(0, 109)\n",
      "(0, 110)\n",
      "(0, 111)\n",
      "(0, 112)\n",
      "(0, 113)\n",
      "(0, 114)\n",
      "(0, 115)\n",
      "(0, 116)\n",
      "(0, 117)\n",
      "(0, 118)\n",
      "(0, 119)\n",
      "(0, 120)\n",
      "(0, 121)\n",
      "(0, 122)\n",
      "(0, 123)\n",
      "(0, 124)\n",
      "(0, 125)\n",
      "(0, 126)\n",
      "(0, 127)\n",
      "(0, 128)\n",
      "(0, 129)\n",
      "(0, 130)\n",
      "(0, 131)\n",
      "(0, 132)\n",
      "(0, 133)\n",
      "(0, 134)\n",
      "(0, 135)\n",
      "(0, 136)\n",
      "(0, 137)\n",
      "(0, 138)\n",
      "(0, 139)\n",
      "(0, 140)\n",
      "(0, 141)\n",
      "(0, 142)\n",
      "(0, 143)\n",
      "(0, 144)\n",
      "(0, 145)\n",
      "(0, 146)\n",
      "(0, 147)\n",
      "(0, 148)\n",
      "(0, 149)\n",
      "(0, 150)\n",
      "(0, 151)\n",
      "(0, 152)\n",
      "(0, 153)\n",
      "(0, 154)\n",
      "(0, 155)\n",
      "(0, 156)\n",
      "(0, 157)\n",
      "(0, 158)\n",
      "(0, 159)\n",
      "(0, 160)\n",
      "(0, 161)\n",
      "(0, 162)\n",
      "(0, 163)\n",
      "(0, 164)\n",
      "(0, 165)\n",
      "(0, 166)\n",
      "(0, 167)\n",
      "(0, 168)\n",
      "(0, 169)\n",
      "(0, 170)\n",
      "(0, 171)\n",
      "(0, 172)\n",
      "(0, 173)\n",
      "(0, 174)\n",
      "(0, 175)\n",
      "(0, 176)\n",
      "(0, 177)\n",
      "(0, 178)\n",
      "(0, 179)\n",
      "(0, 180)\n",
      "(0, 181)\n",
      "(0, 182)\n",
      "(0, 183)\n",
      "(0, 184)\n",
      "(0, 185)\n",
      "(0, 186)\n",
      "(0, 187)\n",
      "(0, 188)\n",
      "(0, 189)\n",
      "(0, 190)\n",
      "(0, 191)\n",
      "(0, 192)\n",
      "(0, 193)\n",
      "(0, 194)\n",
      "(0, 195)\n",
      "(0, 196)\n",
      "(0, 197)\n",
      "(0, 198)\n",
      "(0, 199)\n",
      "(0, 200)\n",
      "(0, 201)\n",
      "(0, 202)\n",
      "(0, 203)\n",
      "(0, 204)\n",
      "(0, 205)\n",
      "(0, 206)\n",
      "(0, 207)\n",
      "(0, 208)\n",
      "(0, 209)\n",
      "(0, 210)\n",
      "(0, 211)\n",
      "(0, 212)\n",
      "(0, 213)\n",
      "(0, 214)\n",
      "(0, 215)\n",
      "(0, 216)\n",
      "(0, 217)\n",
      "(0, 218)\n",
      "(0, 219)\n",
      "(0, 220)\n",
      "(0, 221)\n",
      "(0, 222)\n",
      "(0, 223)\n",
      "(0, 224)\n",
      "(0, 225)\n",
      "(0, 226)\n",
      "(0, 227)\n",
      "(0, 228)\n",
      "(0, 229)\n",
      "(0, 230)\n",
      "(0, 231)\n",
      "(0, 232)\n",
      "(0, 233)\n",
      "(0, 234)\n",
      "(0, 235)\n",
      "(0, 236)\n",
      "(0, 237)\n",
      "(0, 238)\n",
      "(0, 239)\n",
      "(0, 240)\n",
      "(0, 241)\n",
      "(0, 242)\n",
      "(0, 243)\n",
      "(0, 244)\n",
      "(0, 245)\n",
      "(0, 246)\n",
      "(0, 247)\n",
      "(0, 248)\n",
      "(0, 249)\n",
      "(0, 250)\n",
      "(0, 251)\n",
      "(0, 252)\n",
      "(0, 253)\n",
      "(0, 254)\n",
      "(0, 255)\n",
      "(0, 256)\n",
      "(0, 257)\n",
      "(0, 258)\n",
      "(0, 259)\n",
      "(0, 260)\n",
      "(0, 261)\n",
      "(0, 262)\n",
      "(0, 263)\n",
      "(0, 264)\n",
      "(0, 265)\n",
      "(0, 266)\n",
      "(0, 267)\n",
      "(0, 268)\n",
      "(0, 269)\n",
      "(0, 270)\n",
      "(0, 271)\n",
      "(0, 272)\n",
      "(0, 273)\n",
      "(0, 274)\n",
      "(0, 275)\n",
      "(0, 276)\n",
      "(0, 277)\n",
      "(0, 278)\n",
      "(0, 279)\n",
      "(0, 280)\n",
      "(0, 281)\n",
      "(0, 282)\n",
      "(0, 283)\n",
      "(0, 284)\n",
      "(0, 285)\n",
      "(0, 286)\n",
      "(0, 287)\n",
      "(0, 288)\n",
      "(0, 289)\n",
      "(0, 290)\n",
      "(0, 291)\n",
      "(0, 292)\n",
      "(0, 293)\n",
      "(0, 294)\n",
      "(0, 295)\n",
      "(0, 296)\n",
      "(0, 297)\n",
      "(0, 298)\n",
      "(0, 299)\n",
      "(0, 300)\n",
      "(0, 301)\n",
      "(0, 302)\n",
      "(0, 303)\n",
      "(0, 304)\n",
      "(0, 305)\n",
      "(0, 306)\n",
      "(0, 307)\n",
      "(0, 308)\n",
      "(0, 309)\n",
      "(0, 310)\n",
      "(0, 311)\n",
      "(0, 312)\n",
      "(0, 313)\n",
      "(0, 314)\n",
      "(0, 315)\n",
      "(0, 316)\n",
      "(0, 317)\n",
      "(0, 318)\n",
      "(0, 319)\n",
      "(0, 320)\n",
      "(0, 321)\n",
      "(0, 322)\n",
      "(0, 323)\n",
      "(0, 324)\n",
      "(0, 325)\n",
      "(0, 326)\n",
      "(0, 327)\n",
      "(0, 328)\n",
      "(0, 329)\n",
      "(0, 330)\n",
      "(0, 331)\n",
      "(0, 332)\n",
      "(0, 333)\n",
      "(0, 334)\n",
      "(0, 335)\n",
      "(0, 336)\n",
      "(0, 337)\n",
      "(0, 338)\n",
      "(0, 339)\n",
      "(0, 340)\n",
      "(0, 341)\n",
      "(0, 342)\n",
      "(0, 343)\n",
      "(0, 344)\n",
      "(0, 345)\n",
      "(0, 346)\n",
      "(0, 347)\n",
      "(0, 348)\n",
      "(0, 349)\n",
      "(0, 350)\n",
      "(0, 351)\n",
      "(0, 352)\n",
      "(0, 353)\n",
      "(0, 354)\n",
      "(0, 355)\n",
      "(0, 356)\n",
      "(0, 357)\n",
      "(0, 358)\n",
      "(0, 359)\n",
      "(0, 360)\n",
      "(0, 361)\n",
      "(0, 362)\n",
      "(0, 363)\n",
      "(0, 364)\n",
      "(0, 365)\n",
      "(0, 366)\n",
      "(0, 367)\n",
      "(0, 368)\n",
      "(0, 369)\n",
      "(0, 370)\n",
      "(0, 371)\n",
      "(0, 372)\n",
      "(0, 373)\n",
      "(0, 374)\n",
      "(0, 375)\n",
      "(0, 376)\n",
      "(0, 377)\n",
      "(0, 378)\n",
      "(0, 379)\n",
      "(0, 380)\n",
      "(0, 381)\n",
      "(0, 382)\n",
      "(0, 383)\n",
      "(0, 384)\n",
      "(0, 385)\n",
      "(0, 386)\n",
      "(0, 387)\n",
      "(0, 388)\n",
      "(0, 389)\n",
      "(0, 390)\n",
      "(0, 391)\n",
      "(0, 392)\n",
      "(0, 393)\n",
      "(0, 394)\n",
      "(0, 395)\n",
      "(0, 396)\n",
      "(0, 397)\n",
      "(0, 398)\n",
      "(0, 399)\n",
      "(0, 400)\n",
      "(0, 401)\n",
      "(0, 402)\n",
      "(0, 403)\n",
      "(0, 404)\n",
      "(0, 405)\n",
      "(0, 406)\n",
      "(0, 407)\n",
      "(0, 408)\n",
      "(0, 409)\n",
      "(0, 410)\n",
      "(0, 411)\n",
      "(0, 412)\n",
      "(0, 413)\n",
      "(0, 414)\n",
      "(0, 415)\n",
      "(0, 416)\n",
      "(0, 417)\n",
      "(0, 418)\n",
      "(0, 419)\n",
      "(0, 420)\n",
      "(0, 421)\n",
      "(0, 422)\n",
      "(0, 423)\n",
      "(0, 424)\n",
      "(0, 425)\n",
      "(0, 426)\n",
      "(0, 427)\n",
      "(0, 428)\n",
      "(0, 429)\n",
      "(0, 430)\n",
      "(0, 431)\n",
      "(0, 432)\n",
      "(0, 433)\n",
      "(0, 434)\n",
      "(0, 435)\n",
      "(0, 436)\n",
      "(0, 437)\n",
      "(0, 438)\n",
      "(0, 439)\n",
      "(0, 440)\n",
      "(0, 441)\n",
      "(0, 442)\n",
      "(0, 443)\n",
      "(0, 444)\n",
      "(0, 445)\n",
      "(0, 446)\n",
      "(0, 447)\n",
      "(0, 448)\n",
      "(0, 449)\n",
      "(0, 450)\n",
      "(0, 451)\n",
      "(0, 452)\n",
      "(0, 453)\n",
      "(0, 454)\n",
      "(0, 455)\n",
      "(0, 456)\n",
      "(0, 457)\n",
      "(0, 458)\n",
      "(0, 459)\n",
      "(0, 460)\n",
      "(0, 461)\n",
      "(0, 462)\n",
      "(0, 463)\n",
      "(0, 464)\n",
      "(0, 465)\n",
      "(0, 466)\n",
      "(0, 467)\n",
      "(0, 468)\n",
      "(0, 469)\n",
      "(0, 470)\n",
      "(0, 471)\n",
      "(0, 472)\n",
      "(0, 473)\n",
      "(0, 474)\n",
      "(0, 475)\n",
      "(0, 476)\n",
      "(0, 477)\n",
      "(0, 478)\n",
      "(0, 479)\n",
      "(0, 480)\n",
      "(0, 481)\n",
      "(0, 482)\n",
      "(0, 483)\n",
      "(0, 484)\n",
      "(0, 485)\n",
      "(0, 486)\n",
      "(0, 487)\n",
      "(0, 488)\n",
      "(0, 489)\n",
      "(0, 490)\n",
      "(0, 491)\n",
      "(0, 492)\n",
      "(0, 493)\n",
      "(0, 494)\n",
      "(0, 495)\n",
      "(0, 496)\n",
      "(0, 497)\n",
      "(0, 498)\n",
      "(0, 499)\n",
      "(0, 500)\n",
      "(0, 501)\n",
      "(0, 502)\n",
      "(0, 503)\n",
      "(0, 504)\n",
      "(0, 505)\n",
      "(0, 506)\n",
      "(0, 507)\n",
      "(0, 508)\n",
      "(0, 509)\n",
      "(0, 510)\n",
      "(0, 511)\n",
      "(0, 512)\n",
      "(0, 513)\n",
      "(0, 514)\n",
      "(0, 515)\n",
      "(0, 516)\n",
      "(0, 517)\n",
      "(0, 518)\n",
      "(0, 519)\n",
      "(0, 520)\n",
      "(0, 521)\n",
      "(0, 522)\n",
      "(0, 523)\n",
      "(0, 524)\n",
      "(0, 525)\n",
      "(0, 526)\n",
      "(0, 527)\n",
      "(0, 528)\n",
      "(0, 529)\n",
      "(0, 530)\n",
      "(0, 531)\n",
      "(0, 532)\n",
      "(0, 533)\n",
      "(0, 534)\n",
      "(0, 535)\n",
      "(0, 536)\n",
      "(0, 537)\n",
      "(0, 538)\n",
      "(0, 539)\n",
      "(0, 540)\n",
      "(0, 541)\n",
      "(0, 542)\n",
      "(0, 543)\n",
      "(0, 544)\n",
      "(0, 545)\n",
      "(0, 546)\n",
      "(0, 547)\n",
      "(0, 548)\n",
      "(0, 549)\n",
      "(0, 550)\n",
      "(0, 551)\n",
      "(0, 552)\n",
      "(0, 553)\n",
      "(0, 554)\n",
      "(0, 555)\n",
      "(0, 556)\n",
      "(0, 557)\n",
      "(0, 558)\n",
      "(0, 559)\n",
      "(0, 560)\n",
      "(0, 561)\n",
      "(0, 562)\n",
      "(0, 563)\n",
      "(0, 564)\n",
      "(0, 565)\n",
      "(0, 566)\n",
      "(0, 567)\n",
      "(0, 568)\n",
      "(0, 569)\n",
      "(0, 570)\n",
      "(0, 571)\n",
      "(0, 572)\n",
      "(0, 573)\n",
      "(0, 574)\n",
      "(0, 575)\n",
      "(0, 576)\n",
      "(0, 577)\n",
      "(0, 578)\n",
      "(0, 579)\n",
      "(0, 580)\n",
      "(0, 581)\n",
      "(0, 582)\n",
      "(0, 583)\n",
      "(0, 584)\n",
      "(0, 585)\n",
      "(0, 586)\n",
      "(0, 587)\n",
      "(0, 588)\n",
      "(0, 589)\n",
      "(0, 590)\n",
      "(0, 591)\n",
      "(0, 592)\n",
      "(0, 593)\n",
      "(0, 594)\n",
      "(0, 595)\n",
      "(0, 596)\n",
      "(0, 597)\n",
      "(0, 598)\n",
      "(0, 599)\n",
      "(0, 600)\n",
      "(0, 601)\n",
      "(0, 602)\n",
      "(0, 603)\n",
      "(0, 604)\n",
      "(0, 605)\n",
      "(0, 606)\n",
      "(0, 607)\n",
      "(0, 608)\n",
      "(0, 609)\n",
      "(0, 610)\n",
      "(0, 611)\n",
      "(0, 612)\n",
      "(0, 613)\n",
      "(0, 614)\n",
      "(0, 615)\n",
      "(0, 616)\n",
      "(0, 617)\n",
      "(0, 618)\n",
      "(0, 619)\n",
      "(0, 620)\n",
      "(0, 621)\n",
      "(0, 622)\n",
      "(0, 623)\n",
      "(0, 624)\n",
      "(0, 625)\n",
      "(0, 626)\n",
      "(0, 627)\n",
      "(0, 628)\n",
      "(0, 629)\n",
      "(0, 630)\n",
      "(0, 631)\n",
      "(0, 632)\n",
      "(0, 633)\n",
      "(0, 634)\n",
      "(0, 635)\n",
      "(0, 636)\n",
      "(0, 637)\n",
      "(0, 638)\n",
      "(0, 639)\n",
      "(0, 640)\n",
      "(0, 641)\n",
      "(0, 642)\n",
      "(0, 643)\n",
      "(0, 644)\n",
      "(0, 645)\n",
      "(0, 646)\n",
      "(0, 647)\n",
      "(0, 648)\n",
      "(0, 649)\n",
      "(0, 650)\n",
      "(0, 651)\n",
      "(0, 652)\n",
      "(0, 653)\n",
      "(0, 654)\n",
      "(0, 655)\n",
      "(0, 656)\n",
      "(0, 657)\n",
      "(0, 658)\n",
      "(0, 659)\n",
      "(0, 660)\n",
      "(0, 661)\n",
      "(0, 662)\n",
      "(0, 663)\n",
      "(0, 664)\n",
      "(0, 665)\n",
      "(0, 666)\n",
      "(0, 667)\n",
      "(0, 668)\n",
      "(0, 669)\n",
      "(0, 670)\n",
      "(0, 671)\n",
      "(0, 672)\n"
     ]
    }
   ],
   "source": [
    "Tem = compute_TEMfeatures_8day(tem_fs85[0:4], 4, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((3,4))\n",
    "Y = np.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_svmlight_file(X,Y,os.path.join(\"test_sample1\"),zero_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = np.ones((5,4))\n",
    "YY = np.ones(5)\n",
    "dump_svmlight_file(XX,YY,os.path.join(\"test_sample2\"),zero_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.append(Y, [4, 5, 6, 7, 8, 9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.zeros((3,3,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [1.0, 2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0][0] = np.copy(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1. , 2.1],\n",
       "        [0. , 0. ],\n",
       "        [0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. ],\n",
       "        [0. , 0. ],\n",
       "        [0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. ],\n",
       "        [0. , 0. ],\n",
       "        [0. , 0. ]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
